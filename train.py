from unsloth import FastLanguageModel
from transformers import TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset
import torch
import wandb
from huggingface_hub import login
from dotenv import load_dotenv
import os

# Step 1: Initialize environment variables and login
load_dotenv()

hf_token = os.getenv("HUGGINGFACE_TOKEN")
wb_token = os.getenv("WANDB_TOKEN")

# Log into HuggingFace and W&B
login(hf_token)
wandb.login(key=wb_token)

# Initialize W&B project
run = wandb.init(
    project='Qwen-7B-financial-analysis',
    job_type="training",
    anonymous="allow"
)

# 2. Load model and tokenizer (Qwen7B, 4bit)
max_seq_length = 2048
dtype = torch.float16  # Use float16 if you're working with GPUs, else set to None
load_in_4bit = True

model_name = "unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit"
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
    token=hf_token
)

# 3. Define prompt style and pre-finetuning inference test
prompt_style = """ä»¥ä¸‹æ˜¯æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä»¥åŠæä¾›æ›´å¤šä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚
è¯·å†™å‡ºæ°å½“å®Œæˆè¯¥è¯·æ±‚çš„å›ç­”ã€‚
åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€æ­¥çš„æ€ç»´é“¾ï¼Œä»¥ç¡®ä¿å›ç­”åˆä¹é€»è¾‘ä¸”å‡†ç¡®ã€‚

### Instruction:
ä½ æ˜¯ä¸€ä½åœ¨è´¢åŠ¡åˆ†æã€è´¢æŠ¥é˜…è¯»æ–¹é¢å…·æœ‰ä¸“ä¸šçŸ¥è¯†çš„é‡‘èä¸“å®¶ã€‚è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚

### Question:
{}

### Response:
<think>{}</think>
"""

question = "å…¬å¸ï¼šè…¾è®¯ï¼Œè´¢æŠ¥ç±»å‹ï¼š2023å¹´Q4è´¢æŠ¥ã€‚è¯·åˆ†æå…¶è¥æ”¶å’Œå‡€åˆ©æ¶¦çš„ä¸»è¦æ„æˆä¸å˜åŒ–è¶‹åŠ¿ã€‚"

FastLanguageModel.for_inference(model)

inputs = tokenizer(
    [prompt_style.format(question, "")],
    return_tensors="pt"
).to("cuda")

outputs = model.generate(
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_new_tokens=512,
    use_cache=True
)

response = tokenizer.batch_decode(outputs, skip_special_tokens=True)
print("ğŸ” å¾®è°ƒå‰æ¨¡å‹æ¨ç†ç»“æœï¼š")
print(response[0].split("### Response:")[1].strip())

# 4. Load JSONL dataset
def format_fn(example):
    return {
        "text": f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªé‡‘èä¸“å®¶ï¼Œè¯·è¯¦ç»†åˆ†æå…¬å¸çš„è´¢åŠ¡è¡¨ç°ã€‚
<|im_end|>
<|im_start|>user
{example['Question']}
<|im_end|>
<|im_start|>assistant
{example['Complex_CoT']}\n{example['Response']}<|im_end|>
"""
    }

dataset = load_dataset("json", data_files="qwen_financial_data.jsonl", split="train")
dataset = dataset.map(format_fn)

# 5. Configure LoRA parameters
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None,
)

# 6. Configure training arguments
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=20,
        max_steps=50,
        learning_rate=2e-4,
        logging_steps=10,
        fp16=True,  # è®¾ç½® fp16 ä¸º True
        bf16=False, # è®¾ç½® bf16 ä¸º False
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="./outputs",
        report_to="wandb",  # Optional, if you have set up wandb
    ),
)

# 7. Start training
trainer.train()

# 8. Save the model
model.save_pretrained("qwen2.5b-finetuned")
tokenizer.save_pretrained("qwen2.5b-finetuned")
model.save_pretrained_merged(
    save_directory="./qwen-finana-merged",
    tokenizer=tokenizer,
    save_method="merged_16bit",
    push_to_hub=False,
)

#9.test
# æ¨ç†éƒ¨åˆ†ï¼šé‡æ–°ç”Ÿæˆæ¨¡å‹çš„é¢„æµ‹ç»“æœ
question = "å…¬å¸ï¼šè…¾è®¯ï¼Œè´¢æŠ¥ç±»å‹ï¼š2023å¹´Q4è´¢æŠ¥ã€‚è¯·åˆ†æå…¶è¥æ”¶å’Œå‡€åˆ©æ¶¦çš„ä¸»è¦æ„æˆä¸å˜åŒ–è¶‹åŠ¿ã€‚"

# æ¨ç†æ¨¡å¼
FastLanguageModel.for_inference(model)

inputs = tokenizer(
    [prompt_style.format(question, "")],
    return_tensors="pt"
).to("cuda")

outputs = model.generate(
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_new_tokens=512,
    use_cache=True
)

response = tokenizer.batch_decode(outputs, skip_special_tokens=True)
print("ğŸ” å¾®è°ƒåæ¨¡å‹æ¨ç†ç»“æœï¼š")
print(response[0].split("### Response:")[1].strip())

